# Исмагилова Гузель, 11-105

## Задание 1. Проект: Сбор и скачивание текстовых файлов с сайта lib.ru (http://lib.ru/)

1. **Сбор ссылок**:
   - Скрипт проходит по списку авторов и собирает все ссылки на текстовые файлы (`.txt`) с их страниц на сайте `lib.ru`.
   - Собранные ссылки сохраняются в файл `urls.txt`.

2. **Скачивание страниц**:
   - Скрипт скачивает страницы по собранным ссылкам и сохраняет их в папку `downloaded_pages`.
   - Для каждой скачанной страницы создается запись в файле `index.txt`, которая связывает номер страницы с её URL.

## Как использовать

### 1. Установите необходимые библиотеки:

  ```bash
  pip install requests beautifulsoup4
  ```

### 2. Клонируйте репозиторий:

  ```bash
  git clone https://github.com/lismagilova/searcher.git cd <ваш-репозиторий>
  ```

### 3. Запуск скрипта
  
  ```bash
  python first/pages-extraction.py
  ```

### Скрипт выполнит следующие действия:

- Соберет ссылки на текстовые файлы и сохранит их в urls.txt.
- Скачает страницы и сохранит их в папку downloaded_pages.
- Создаст файл index.txt для индексации скачанных страниц.

### Результаты работы
- urls.txt: Файл с собранными ссылками.
- downloaded_pages: Папка с скачанными HTML-страницами/zip-архив.
- index.txt: Файл с индексацией страниц и их URL.

## Задание 2. Проект: Токенизация и лемматизация текстовых файлов

1. **Обработка HTML-файлов**:
   - Скрипт проходит по всем HTML-файлам в папке `first/downloaded_pages`.
   - Очищает их от HTML-разметки.
   - Выделяет уникальные слова (токены), исключая предлоги, союзы, числа и мусорные данные.
2. **Лемматизация**:
   - Преобразует токены в их начальную форму (леммы).
   - Группирует слова по их леммам.
3. **Сохранение результатов**:
   - Список уникальных токенов сохраняется в файл `tokens.txt`.
   - Лемматизированные слова с группировкой по леммам сохраняются в `lemmas.txt`.

## Как использовать

### 1. Установите необходимые библиотеки

```bash
pip install beautifulsoup4 pymorphy2 html5lib
```

### 2. Запустите скрипт

```bash
python second/tokenization-lemmatization.py
```

### Скрипт выполнит следующие действия:

- Прочитает все HTML-файлы в папке `documents`.
- Выделит токены и удалит ненужные элементы.
- Лемматизирует токены и сгруппирует их.
- Сохранит результаты в файлы `tokens.txt` и `lemmas.txt`.

### Результаты работы

- `tokens.txt`: список уникальных токенов.
- `lemmas.txt`: список лемм с привязанными к ним токенами.

