# Исмагилова Гузель, 11-105

## Задание 1. Проект: Сбор и скачивание текстовых файлов с сайта lib.ru (http://lib.ru/)

1. **Сбор ссылок**:
   - Скрипт проходит по списку авторов и собирает все ссылки на текстовые файлы (`.txt`) с их страниц на сайте `lib.ru`.
   - Собранные ссылки сохраняются в файл `urls.txt`.

2. **Скачивание страниц**:
   - Скрипт скачивает страницы по собранным ссылкам и сохраняет их в папку `downloaded_pages`.
   - Для каждой скачанной страницы создается запись в файле `index.txt`, которая связывает номер страницы с её URL.

## Как использовать

### 1. Установите необходимые библиотеки:

  ```bash
  pip install requests beautifulsoup4
  ```

### 2. Клонируйте репозиторий:

  ```bash
  git clone https://github.com/lismagilova/searcher.git cd <ваш-репозиторий>
  ```

### 3. Запуск скрипта
  
  ```bash
  python first/pages-extraction.py
  ```

### Скрипт выполнит следующие действия:

- Соберет ссылки на текстовые файлы и сохранит их в urls.txt.
- Скачает страницы и сохранит их в папку downloaded_pages.
- Создаст файл index.txt для индексации скачанных страниц.

### Результаты работы
- `urls.txt` - файл с собранными ссылками.
- `downloaded_pages` - папка с скачанными HTML-страницами/zip-архив.
- `index.txt` - файл с индексацией страниц и их URL.

## Задание 2. Проект: Токенизация и лемматизация текстовых файлов

1. **Обработка HTML-файлов**:
   - Скрипт проходит по всем HTML-файлам по отдельности в папке `first/downloaded_pages`.
   - Очищает их от HTML-разметки.
   - Выделяет уникальные слова (токены), исключая предлоги, союзы, числа и мусорные данные.
2. **Лемматизация**:
   - Преобразует токены в их начальную форму (леммы).
   - Группирует слова по их леммам.
3. **Сохранение результатов**:
   - Список уникальных токенов сохраняется в файл `tokens_n.txt`.
   - Лемматизированные слова с группировкой по леммам сохраняются в `lemmas_n.txt`.

## Как использовать

### 1. Установите необходимые библиотеки

```bash
pip install beautifulsoup4 pymorphy2 html5lib
```

### 2. Запустите скрипт

```bash
python second/tokenization-lemmatization.py
```

### Скрипт выполнит следующие действия:

- Прочитает все HTML-файлы в папке `documents`.
- Выделит токены и удалит ненужные элементы.
- Лемматизирует токены и сгруппирует их.
- Сохранит результаты в файлы `tokens_n.txt` и `lemmas_n.txt`.

### Результаты работы

- `tokens/` - для файлов с токенами
- `lemmas/` - для файлов с леммами

## Задание 3. Проект: Булев поиск по документам
1. **Построение инвертированного индекса:**
   - На основе токенизированных документов из папки second/tokens строится инвертированный индекс.
   - Индекс сохраняется в файл `third/inverted_index.json`.

2. **Обработка поисковых запросов:**
   - Поддержка булевых операторов `(AND, OR, NOT)` и скобок для группировки.
   - Преобразование лемм в запросе в соответствующие термины.
   - Расширение запроса с учетом синонимичных терминов.

3. **Выполнение поиска:**
   - Поиск документов, соответствующих запросу.
   - Вывод списка найденных документов с их названиями.

## Как использовать
### 1. Установите необходимые библиотеки
```bash
pip install pymorphy2
```
### 2. Запустите скрипт
```bash
python third/boolean_search.py
```
### Скрипт выполнит следующие действия:
- Построит или загрузит инвертированный индекс из файла inverted_index.json

- Предоставит интерактивную консоль для ввода поисковых запросов

- Обработает запрос с учетом булевой логики и лемматизации

- Выведет список документов, соответствующих запросу

### Результаты работы

- `inverted_index.json` - инвертированный индекс