# Исмагилова Гузель, 11-105

## Задание 1. Проект: Сбор и скачивание текстовых файлов с сайта lib.ru (http://lib.ru/)

1. **Сбор ссылок**:
   - Скрипт проходит по списку авторов и собирает все ссылки на текстовые файлы (`.txt`) с их страниц на сайте `lib.ru`.
   - Собранные ссылки сохраняются в файл `urls.txt`.

2. **Скачивание страниц**:
   - Скрипт скачивает страницы по собранным ссылкам и сохраняет их в папку `downloaded_pages`.
   - Для каждой скачанной страницы создается запись в файле `index.txt`, которая связывает номер страницы с её URL.

## Как использовать

### 1. Установите необходимые библиотеки:

  ```bash
  pip install requests beautifulsoup4
  ```

### 2. Клонируйте репозиторий:

  ```bash
  git clone https://github.com/lismagilova/searcher.git cd <ваш-репозиторий>
  ```

### 3. Запуск скрипта
  
  ```bash
  python first/pages-extraction.py
  ```

### Скрипт выполнит следующие действия:

- Соберет ссылки на текстовые файлы и сохранит их в urls.txt.
- Скачает страницы и сохранит их в папку downloaded_pages.
- Создаст файл index.txt для индексации скачанных страниц.

### Результаты работы
- `urls.txt` - файл с собранными ссылками.
- `downloaded_pages` - папка с скачанными HTML-страницами/zip-архив.
- `index.txt` - файл с индексацией страниц и их URL.

## Задание 2. Проект: Токенизация и лемматизация текстовых файлов

1. **Обработка HTML-файлов**:
   - Скрипт проходит по всем HTML-файлам по отдельности в папке `first/downloaded_pages`.
   - Очищает их от HTML-разметки.
   - Выделяет уникальные слова (токены), исключая предлоги, союзы, числа и мусорные данные.
2. **Лемматизация**:
   - Преобразует токены в их начальную форму (леммы).
   - Группирует слова по их леммам.
3. **Сохранение результатов**:
   - Список уникальных токенов сохраняется в файл `tokens_n.txt`.
   - Лемматизированные слова с группировкой по леммам сохраняются в `lemmas_n.txt`.

## Как использовать

### 1. Установите необходимые библиотеки

```bash
pip install beautifulsoup4 pymorphy2 html5lib
```

### 2. Запустите скрипт

```bash
python second/tokenization-lemmatization.py
```

### Скрипт выполнит следующие действия:

- Прочитает все HTML-файлы в папке `documents`.
- Выделит токены и удалит ненужные элементы.
- Лемматизирует токены и сгруппирует их.
- Сохранит результаты в файлы `tokens_n.txt` и `lemmas_n.txt`.

### Результаты работы

- `tokens/` - для файлов с токенами
- `lemmas/` - для файлов с леммами

## Задание 3. Проект: Булев поиск по документам
1. **Построение инвертированного индекса:**
   - На основе токенизированных документов из папки second/tokens строится инвертированный индекс.
   - Индекс сохраняется в файл `third/inverted_index.json`.

2. **Обработка поисковых запросов:**
   - Поддержка булевых операторов `(AND, OR, NOT)` и скобок для группировки.
   - Преобразование лемм в запросе в соответствующие термины.
   - Расширение запроса с учетом синонимичных терминов.

3. **Выполнение поиска:**
   - Поиск документов, соответствующих запросу.
   - Вывод списка найденных документов с их названиями.

## Как использовать

### 1. Запустите скрипт
```bash
python third/boolean_search.py
```
### Скрипт выполнит следующие действия:
- Построит или загрузит инвертированный индекс из файла inverted_index.json
- Предоставит интерактивную консоль для ввода поисковых запросов
- Обработает запрос с учетом булевой логики и лемматизации
- Выведет список документов, соответствующих запросу

### Результаты работы

- `inverted_index.json` - инвертированный индекс


## Задание 4. Проект: Расчет TF-IDF для терминов и лемм
1. **Обработка токенов и лемм:**
   - Скрипт проходит по всем файлам с токенами из папки `second/tokens/` и леммам из `second/lemmas/`.
   - Для каждого документа рассчитывает TF (частота термина) по формуле:
`TF = (количество вхождений термина в документе) / (общее количество терминов в документе)`
   - Для каждого термина/леммы рассчитывает IDF по формуле:
`IDF = log(общее количество документов / количество документов, содержащих термин)`

2. **Расчет TF-IDF:**
   - Для каждого термина и леммы в каждом документе вычисляет TF-IDF как произведение TF и IDF.
   - Сохраняет результаты в двух отдельных папках.

3. **Сохранение результатов:**
   - Для терминов: файлы tf_idf_terms_n.txt в папке fourth/terms/
   - Для лемм: файлы tf_idf_lemmas_n.txt в папке fourth/lemmas/

## Как использовать

### 1. Запустите скрипт

```bash
python fourth/tfidf_calculation.py
```

### Скрипт выполнит следующие действия:
- Загрузит все токены и леммы из соответствующих папок
- Рассчитает TF, IDF и TF-IDF для каждого термина и леммы
- Сохранит результаты в файлы с нумерацией, соответствующей документам

### Результаты работы

- `fourth/terms/` - TF-IDF файлов терминов
- `fourth/lemmas/` - TF-IDF файлов лемматизированных форм


## Задание 5. Проект: Векторная поисковая система
1. **Загрузка данных:**
   - Использует TF-IDF значения из папки fourth/terms/
   - Поддерживает различные форматы имен файлов документов

2. **Обработка запросов:**
   - Удаление стоп-слов (предлоги, союзы)
   - Нормализация запроса (приведение к нижнему регистру)

3. **Поиск и ранжирование:**
   - Построение векторного представления запроса
   - Расчет косинусной близости с документами
   - Сортировка по убыванию релевантности

4. **Вывод результатов:**
   - Интерактивный режим работы
   - Отображение ID документа и оценки релевантности

## Как использовать
### 1. Установите необходимые зависимости:

```bash
pip install numpy scikit-learn
```

### 2. Запустите поисковую систему:

```bash
python fifth/web_search.py
```

### 3. Вводите поисковые запросы в интерактивном режиме, например:

```bash
поиск война и мир
русская литература XIX века
```

### 4. Для выхода введите:

```bash
exit
```
